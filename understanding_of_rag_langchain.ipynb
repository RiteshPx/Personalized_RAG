{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb openai tiktoken -q\n",
        "!pip install chromadb  sentence-transformers -qU\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "X6EwlFinYVmf"
      },
      "id": "X6EwlFinYVmf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# You can use any free model here\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def get_hf_embedding(text):\n",
        "    return model.encode(text).tolist()\n"
      ],
      "metadata": {
        "id": "VApGW-3ma6Kq"
      },
      "id": "VApGW-3ma6Kq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "client = chromadb.Client(Settings(\n",
        "    persist_directory=\"./chroma_db\"  # saves to local folder\n",
        "))\n",
        "\n",
        "collection = client.get_or_create_collection(name=\"knowledge_base\")\n",
        "\n",
        "# Example texts to store\n",
        "texts = [\n",
        "    \"Python is a versatile programming language.\",\n",
        "    \"LangChain is used to build LLM-based applications.\",\n",
        "    \"ChromaDB is a lightweight open-source vector database.\"\n",
        "]\n",
        "\n",
        "# Choose one embedding method (Gemini or HuggingFace)\n",
        "embeddings = [get_hf_embedding(t) for t in texts]  # or get_gemini_embedding(t)\n",
        "\n",
        "collection.add(\n",
        "    documents=texts,\n",
        "    ids=[f\"id{i}\" for i in range(len(texts))],\n",
        "    embeddings=embeddings\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Data inserted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ickYHl-aaFNC",
        "outputId": "6e062835-9d2d-468e-e8dd-7a9458ff1611"
      },
      "id": "ickYHl-aaFNC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data inserted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is ChromaDB used for?\"\n",
        "query_embedding = get_hf_embedding(query)  # or get_gemini_embedding(query)\n",
        "\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "for doc in results['documents'][0]:\n",
        "    print(\"üîπ\", doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odarYp0Qan1h",
        "outputId": "27ce57b7-60b6-4795-a194-a61f207b88be"
      },
      "id": "odarYp0Qan1h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ ChromaDB is used to store and retrieve text embeddings efficiently.\n",
            "üîπ ChromaDB is a lightweight open-source vector database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lanchain"
      ],
      "metadata": {
        "id": "kwS2vK8lbQ-k"
      },
      "id": "kwS2vK8lbQ-k"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb sentence-transformers google-generativeai langchain-core langchain-community -qU\n"
      ],
      "metadata": {
        "id": "A8cCPKNxbGJQ"
      },
      "id": "A8cCPKNxbGJQ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_text(texts):\n",
        "    return embedding_model.encode(texts).tolist()\n"
      ],
      "metadata": {
        "id": "aAasv-UdbcwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "e69e427b-5d24-43c9-82d8-070da4890d87"
      },
      "id": "aAasv-UdbcwL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3696276813.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0;31m# A model from sentence-transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             has_modules = is_sentence_transformer_model(\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/util/file_io.py\u001b[0m in \u001b[0;36mis_sentence_transformer_model\u001b[0;34m(model_name_or_path, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[1;32m     52\u001b[0m     return bool(\n\u001b[0;32m---> 53\u001b[0;31m         load_file_path(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;34m\"modules.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/util/file_io.py\u001b[0m in \u001b[0;36mload_file_path\u001b[0;34m(model_name_or_path, filename, subfolder, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         return hf_hub_download(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m     hf_headers = build_hf_headers(\n\u001b[0m\u001b[1;32m    970\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0mlibrary_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlibrary_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, library_name, library_version, user_agent, headers, is_write_action)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Get auth token to send\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mtoken_to_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_to_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Combine headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mget_token_to_send\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Token is not provided: we get it from local cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mcached_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Case token is explicitly required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py\u001b[0m in \u001b[0;36mget_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_token_from_google_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_get_token_from_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_get_token_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py\u001b[0m in \u001b[0;36m_get_token_from_google_colab\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0m_GOOGLE_COLAB_SECRET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;31m# thread-safe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0m_userdata_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     resp = _message.blocking_request(\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;34m'GetSecret'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "client = chromadb.Client(Settings(persist_directory=\"./chroma_rag\"))\n",
        "collection = client.get_or_create_collection(\"rag_knowledge\")\n",
        "\n",
        "# Add some data (knowledge base)\n",
        "documents = [\n",
        "    \"LangChain is a framework to build applications powered by large language models.\",\n",
        "    \"ChromaDB is used to store and retrieve text embeddings efficiently.\",\n",
        "    \"Retrieval-Augmented Generation combines document retrieval with text generation.\",\n",
        "    \"HuggingFace models can generate embeddings locally without using an API key.\"\n",
        "]\n",
        "\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    ids=[f\"doc_{i}\" for i in range(len(documents))],\n",
        "    embeddings=embed_text(documents)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Knowledge base created successfully!\")\n"
      ],
      "metadata": {
        "id": "6k0w3iLsbkVc"
      },
      "id": "6k0w3iLsbkVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-chroma -q"
      ],
      "metadata": {
        "id": "WJ0ZTOF0boND"
      },
      "id": "WJ0ZTOF0boND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.vectorstores import Chroma\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "class HFEmbedding(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        return embed_text(texts)\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return embed_text([text])[0]\n",
        "\n",
        "# Create retriever\n",
        "vectorstore = Chroma(\n",
        "    client=client,\n",
        "    collection_name=\"rag_knowledge\",\n",
        "    embedding_function=HFEmbedding()\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n"
      ],
      "metadata": {
        "id": "vKqHWX3eb_I4"
      },
      "id": "vKqHWX3eb_I4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD9caRAI6oChsXLC7uDWB8FcSPcEP_oixg\""
      ],
      "metadata": {
        "id": "-Seu4dR3nvyk"
      },
      "id": "-Seu4dR3nvyk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyD9caRAI6oChsXLC7uDWB8FcSPcEP_oixg\")\n",
        "\n",
        "def generate_answer(context, question):\n",
        "    prompt = f\"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "    Question: {question}\"\"\"\n",
        "\n",
        "    response = genai.GenerativeModel(\"gemini-2.5-flash\").generate_content(prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "Ifc8VId3cYWP"
      },
      "id": "Ifc8VId3cYWP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer(\"age i just 13\",'waht is my age')"
      ],
      "metadata": {
        "id": "Yq_8g3bZdwrm"
      },
      "id": "Yq_8g3bZdwrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(question):\n",
        "    # Step 1: Retrieve relevant docs\n",
        "    docs = retriever.invoke(question)\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    # print(context)\n",
        "    # Step 2: Generate answer using Gemini\n",
        "    answer = generate_answer(context, question)\n",
        "    return answer\n",
        "\n",
        "# Example Query\n",
        "response = ask_question(\"What is the purpose of ChromaDB?\")\n",
        "print(\"ü§ñ Answer:\", response)"
      ],
      "metadata": {
        "id": "nIhe1csSdMWT"
      },
      "id": "nIhe1csSdMWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collection.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVpGtNM9pEdn",
        "outputId": "92baf22f-6073-41a9-f099-a57cf028985a"
      },
      "id": "pVpGtNM9pEdn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pdf and file with langchain"
      ],
      "metadata": {
        "id": "WT-Zpb_UNyVE"
      },
      "id": "WT-Zpb_UNyVE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb sentence-transformers pypdf langchain-community google-generativeai -qU\n"
      ],
      "metadata": {
        "id": "Bh8I-h0dqCK0"
      },
      "id": "Bh8I-h0dqCK0",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chromadb\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.embeddings.base import Embeddings"
      ],
      "metadata": {
        "id": "n2svYDV6PH9W"
      },
      "id": "n2svYDV6PH9W",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your file\n",
        "pdf_path = \"/content/major_proj_report.pdf\"\n",
        "\n",
        "# Load PDF pages\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(documents)} pages from {pdf_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P12dKj9PPMO",
        "outputId": "d0603909-47ce-4201-d39e-a217a7b69ea7"
      },
      "id": "-P12dKj9PPMO",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 66 pages from /content/major_proj_report.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cnR47bamP8cF",
        "outputId": "978285f9-df99-460f-c32d-fae946925697"
      },
      "id": "cnR47bamP8cF",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 0, 'page_label': '1'}, page_content='AI VISION TRACKER \\n \\nA \\nMinor Project Report \\nSubmitted in partial fulfillment of the requirement for the award of degree of \\n \\nBachelor of Technology \\nIn \\nComputer Science & Engineering \\n \\nSubmitted to \\nRAJIV GANDHI PROUDYOGIKI VISHWAVIDYALAYA,  \\nBHOPAL (M.P.) \\n \\n \\nGuided by                Submitted By  \\nMr. Paras Bhanopiya                                                           Pooja  Parmar (0832CS221150)  \\nAssistant professor               Ritesh Parmar (0832CSS221165) \\n                                        Ritika Mahajan(0832CS221166)  \\n         \\n  \\n \\n     \\nDEPARTMENT OF COMPUTER SCIENCE & ENGINEERING \\nCHAMELI DEVI GROUP OF INSTITUTIONS \\nINDORE (M.P.) 452020 \\n2025-26'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 1, 'page_label': '2'}, page_content='AI VISION TRACKER \\n \\nA Major Project Report submitted to \\n Rajiv Gandhi Proudyogiki Vishwavidyalaya, Bhopal \\n Bachelor of Technology  \\nin \\nComputer Science and Engineering \\nby \\nPooja Parmar (0832CSS221150)                                                                                                                                                                      \\nRitesh Parmar (0832CS221165) \\n      Ritika Mahajan (0832CS221166)  \\n \\n \\n           Under the guidance of   \\n \\n             Mr. Paras Bhanopiya            \\n  (Assistant professor) \\n  \\n \\n Session: 2025-2026 \\n \\nDepartment of Computer Science & Engineering \\nChameli Devi Group of Institutions, \\n Indore 452020  \\n(Madhya Pradesh)'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 2, 'page_label': '3'}, page_content='I \\n \\nDECLARATION    \\n  \\nWe certify that the work contained in this report is original and has been done by us under the \\nguidance of my supervisor(s).  \\na. The work has not been submitted to any other Institute for any degree or diploma.  \\nb. We have followed the guidelines provided by the Institute in preparing the report.  \\nc. We have conformed to the norms and guidelines given in the Ethical Code of Conduct of the \\nInstitute.  \\nd. Whenever we have used materials (data, theoretical analysis, figures, and text) from other \\nsources, we have given due credit to them by citing them in the text of the report and giving \\ntheir details in the references.    \\n  \\n  \\nName and Signature of Project Team Members:  \\n  \\nSr. \\nNo.  Enrollment No.  Name of students  Signature of students  \\n1.  Pooja Parmar  0832CS221150   \\n2.  Ritesh Parmar  0832CS221165   \\n3.  Ritika Mahajan  0832CS221166'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 3, 'page_label': '4'}, page_content='II \\n \\nCHAMELI DEVI GROUP OF INSTITUTIONS, INDORE  \\n  \\n \\n   \\nCERTIFICATE  \\n  \\n  \\nCertified that the Major project report entitled, AI VISION TRACKER is a bonafide work done under \\nmy guidance by Pooja Parmar, Ritesh Parmar, Ritika Mahajan in partial fulfillment of the \\nrequirements for the award of degree of Bachelor of Technology in Computer Science and \\nEngineering.  \\n  \\nDate:                           _________________________  \\n                                                                                                                         (Prof. Paras Bhanopiya )  \\n             Guide  \\n    \\n__________________________             _________________________  \\n(Prof. Radheshyam Acholiya)                      (Dr. Manoj Agrawal)  \\n     Head of the Department    \\n  \\n                     Project Coordinator  \\n __________________________             _________________________  \\n     (Dr. Manish Shrivastva)                                                (                                      )  \\n         Principal, CDGI     \\n  \\n                                       (External)'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 4, 'page_label': '5'}, page_content='III \\n \\n \\nCHAMELI DEVI GROUP OF INSTITUTIONS, INDORE \\n \\nACKNOWLEDGEMENT \\n \\nWe have immense pleasure in expressing our sincerest and deepest sense of gratitude towards \\nour guide Prof. Paras Bhanopiya  for the assistance, valuable guidance and cooperation in \\ncarrying out this Project work. We express our sincere gratitude to Dr. Manoj Agrawal  \\n(Associate Professor, Dept. of CSE, Major Project Coordinator) for his valuable guidance and \\nconstant support. We are developing this project with the help of Faculty members of our \\ninstitute and we are extremely grateful to all of them. We also take this opportunity to thank \\nthe  Head of the Department Prof. Radheshyam Acholiya , and the Principal of Chameli \\nDevi Group of Institutions Dr. Manish Shrivastava, for providing the required facilities for \\nthe project work. We are greatly thankful to our parents, friends and faculty members for \\ntheir motivation, guidance and help whenever needed. \\n \\n \\nName and signature of team Members: \\n \\n1. Pooja Parmar  ‚Ä¶..‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶. \\n2. Ritesh Parmar ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶. \\n3. Ritika Mahajan .‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 5, 'page_label': '6'}, page_content='IV \\n \\nList of Figures \\n \\n \\n \\nList of Tables \\n \\n \\nFigure no. Name of Figure Page Number \\nFigure 1 Block Diagram 21 \\nFigure 2 Level 0 DFD 24 \\nFigure 3 Level 1 DFD 24 \\nFigure 4 Use case Diagram 25 \\nFigure 5 Class Diagram 27 \\nFigure 6 Entity Relationship Diagram 29 \\nFigure 7 Table Structure 30 \\nFigure 8 Level 0 DFD 35 \\nTables no. Name of  Tables Page Number \\nTable 1 Literature Review 21 \\nFigure 2 Zone  Dictionary 30 \\nFigure 3 Camera Dictionary 30-31 \\nFigure 4 Detection Dictionary 32 \\nFigure 5 Anomaly Dictionary 32-33 \\nFigure 6 Alert Dictionary 33 \\nFigure 7 User Dictionary 33-34 \\nFigure 8 EventLog  Dictionary 34'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 6, 'page_label': '7'}, page_content='V \\n \\nList of Publications \\nDuring the development and research phase of this project, the following papers and \\nmaterials were referenced or inspired publications: \\n1. ‚ÄúYOLOv8: Real-Time Object Detection and Tracking‚Äù ‚Äì International Journal of \\nComputer Vision, 2023. \\n2. ‚ÄúDeep Learning-Based Surveillance Systems for Smart Cities‚Äù ‚Äì IEEE Access, \\n2022. \\n3. ‚ÄúVehicle Intrusion Detection in Restricted Zones Using AI and IoT‚Äù ‚Äì Springer \\nAI Research Journal, 2024. \\n4. ‚ÄúAutomated Anomaly Detection Using CNN and RNN Architectures‚Äù ‚Äì Elsevier \\nComputer Science Review, 2023. \\n5. ‚ÄúIntegration of AI with Urban Security Systems‚Äù ‚Äì Journal of Intelligent Systems, \\n2021.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 7, 'page_label': '8'}, page_content='VI \\n \\nAbstract \\nIn urban environments, ensuring safety and discipline within restricted areas such as \\npedestrian zones, school campuses, and hospital corridors is essential. Unauthorized vehicle \\nentry into these ‚ÄúNo Vehicle Zones‚Äù can lead to congestion, accidents, and potential harm to \\npedestrians. \\nTo address this challenge, the proposed system introduces an AI-powered surveillance \\nsolution capable of automatically detecting vehicle -related anomalies using computer vision \\nand deep learning. \\nThe system captures video streams from CCTV cameras, processes them through pre -trained \\nobject detection models like YOLOv8 or MobileNet SSD, and identifies vehicles intruding \\ninto restricted zones. Upon detection, the system immediately flags the event, stores the log \\ndata, and triggers real-time alerts via visual or email notifications. \\nAdditionally, a web-based dashboard  allows administrators to mo nitor incidents, review \\nevent logs, and configure restricted areas dynamically.  \\nThe overall goal of this system is to enhance safety, reduce manual monitoring errors, \\nand promote automation  in public security management through the integration of AI, \\ncomputer vision, and real-time analytics.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 8, 'page_label': '9'}, page_content='VII \\n \\n                                            TABLE OF CONTENTS  \\nCONTENTS  Page No.  \\nTitle Page    \\nDeclaration  I  \\nCertificate by the Supervisor  II  \\nAcknowledgement  III  \\nList of Figures  IV  \\nList of Tables  IV \\nList of Publications  V \\nAbstract  VI \\nChapter 1: Introduction   1- 8 \\n1.1 Rationale   1  \\n1.2 Project Overview   1 \\n1.3 Objective    2 \\n1.4 Scope   2  \\n1.5 Methodology   3 \\n1.6 Roles & Responsibilities  4 \\n       1.7 Contribution of Project   5-6 \\n1.7.1 Market Potential             5 \\n1.7.2 Innovativeness   6  \\n1.7.3 Usefulness   6 \\n1.8 Report Organization   7-8  \\nChapter 2: Literature Review  9-11  \\nChapter 3: Software Requirement Specifications (SRS)   12- 18 \\n       3.1 Introduction   12  \\n             3.1.1 Product Overview  12'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 9, 'page_label': '10'}, page_content='VIII \\n \\n       3.2 Software Functional Requirements   13 \\n 3.2.1 Distributed Database or Client Server Model    13  \\n       3.3 Non-Functional Requirement      14-16 \\n 3.3.1 Reliability  14  \\n 3.3.2 Availability  14  \\n 3.3.3 Security  15  \\n 3.3.4 Maintainability  15  \\n 3.2.5 Portability  16  \\n 3.2.6 Performance   16  \\n       3.4 External Interface Requirement   17-18  \\n3.4.1 User Interface   17  \\n3.4.2 Hardware Interface   17  \\n3.4.3 Software Interface   18  \\n3.4.4 Communication Interface    18 \\nChapter 4: Methodology  19- 22 \\n       4.1 Data Collection       19  \\n       4.2 Data Preprocessing       19  \\n       4.3 Dataset Details        20 \\n       4.4 Training & Experimentation on Dataset       20  \\n       4.5 Block Diagram of Methodology        21 \\n       4.6 Details of Proposed Model for Implementation       22  \\n       4.7 Performance Evaluation Measures        22 \\nChapter 5: Software Design Description (SDD)   23-43  \\n       5.1 Introduction   .23  \\n       5.2 Design Overview  23 -27 \\n   5.2.1 Data Flow Diagram  23- 24'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 10, 'page_label': '11'}, page_content='IX \\n \\n   5.2.2 Use-case Diagram  25  \\n   5.2.3 Class Diagrams        26-27 \\n       5.3 Database Design  28- 34 \\n   5.3.1 Entity Relationship Diagram  28-29  \\n   5.3.2 Normalization Details  29  \\n   5.3.3 Table Structure  30  \\n   5.3.4 Data Dictionary   30-34 \\n       5.4 Data Flow Diagram   34-35 \\n       5.5 System Architectural Design  35-37  \\n       5.6 Detailed Description of Components (Modules)   37-40 \\n             5.6.1 Module 1  37-38  \\n             5.6.2 Module 2  38  \\n             5.6.3 Module 3 39  \\n             5.6.4 Module 4 39-40 \\n       5.7 User Interface Design   40-43  \\n             5.7.1 Description of the User Interface  40-41  \\n             5.7.2 Screen Images  42-43 \\nChapter 6: Software Test Documentation (STD)  44-48 \\n       6.1 Introduction          44 \\n64.1.1 System Overview   44 \\n 6.1.2 Test Approach  44 \\n             6.1.3 Testing Objectives   45 \\n       6.2 Test Plan   45- 46 \\n             6.2.1 Features to be Tested   45 \\n             6.2.2 Features not to be Tested  46  \\n       6.3 Test Cases  46- 48'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 11, 'page_label': '12'}, page_content='X \\n \\n               6.3.1 Unit Testing   46-48 \\n               6.3.2 Functional Testing   47  \\n               6.3.3 System Testing  47  \\n               6.3.4 Integration Testing   48 \\n               6.3.5 Validation Testing  48  \\nChapter 7: Conclusion and Future Scope  49  \\nReferences  50  \\nAppendix-A   \\n51-54  Appendix-B  \\nAppendix-C'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 12, 'page_label': '13'}, page_content='Introduction \\n \\nAI Vision Tracker \\n 1        \\n \\nChapter-1 \\nIntroduction  \\n \\n1.1 Rationale \\nIn urban environments, maintaining safety and discipline within restricted areas such as \\npedestrian-only zones, school premises, and hospital corridors is crucial. Unauthorized \\nvehicle entry into these areas can cause accidents, congestion, or potential harm to \\npedestrians. Traditional manual monitoring systems often fail to detect such violations in \\nreal time due to human limitations. \\nThis project introduces an AI-powered surveillance system designed specifically to detect \\nvehicle-related anomalies within No Vehicle Zones. Using advanced computer vision and \\ndeep learning techniques, the system automatically identifies and alerts authorities when \\na vehicle enters a restricted zone, ensuring a safer and more organized environment.. \\n1.2 Project Overview \\nThe AI Vision Tracker focuses on detecting anomalies related to vehicle intrusion in no-\\nvehicle areas through video surveillance. The system processes real-time camera feeds or \\nrecorded footage, identifies moving objects, classifies them (vehicle vs. pedestrian), and \\nraises alerts when a vehicle is detected in a restricted zone. \\nThe model leverages pre-trained object detection architectures such as YOLOv8 or \\nMobileNet SSD for precise vehicle identification and region-based filtering to define \\nzone boundaries. The system can be deployed in places like school campuses, parks, \\nhospital entrances, and marketplaces to automatically monitor violations. \\n.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 13, 'page_label': '14'}, page_content='Introduction \\nAI Vision Tracker \\n 2 \\n  \\n \\n1.3 Objective \\n1. To design and develop an AI-based surveillance system for No Vehicle Zone \\nanomaly detection. \\n2. To detect and classify vehicles entering restricted pedestrian zones using \\ncomputer vision and deep learning. \\n3. To automate alert generation (visual/audio/email) upon vehicle detection in a \\nrestricted area. \\n4. To minimize dependency on manual surveillance and human monitoring errors. \\n5. To provide an easy-to-use web dashboard for reviewing flagged incidents with \\ntimestamps and video snapshots. \\n \\n1.4 Scope  \\nThe project is limited to detecting vehicle presence in designated No Vehicle Zones. It \\nfocuses on real-time monitoring using CCTV feeds or recorded videos. \\n‚Ä¢ Applicable in schools, hospitals, marketplaces, and government areas with \\nrestricted access. \\n‚Ä¢ Supports both fixed-camera and drone-based video input. \\n‚Ä¢ Scalable for integration with existing security infrastructure. \\n‚Ä¢ Future expansion can include automated number plate recognition (ANPR) and \\nintegration with municipal traffic control systems.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 14, 'page_label': '15'}, page_content='Introduction \\nAI Vision Tracker \\n 3 \\n  \\n \\n1.5  Methodology \\n1. Video Input & Preprocessing:                                                                                   \\nCapture live or recorded video feeds and preprocess them using OpenCV (frame \\nresizing, normalization, and noise reduction). \\n2. Object Detection: \\nApply YOLOv8 or MobileNet SSD models trained on vehicle datasets (cars, \\nbikes, etc.) to detect and classify moving objects. \\n3. Zone Definition: \\nDefine the No Vehicle Zone as a polygonal region within the frame; detections \\nwithin this region trigger anomaly checks. \\n4. Anomaly Identification: \\nIf a vehicle object is detected inside the restricted zone, it is flagged as an \\nanomaly.              \\n5. Alert Generation & Logging: \\nStore event details (timestamp, frame, object type) in a database and trigger real-\\ntime notifications via the web panel.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 15, 'page_label': '16'}, page_content='Introduction \\nAI Vision Tracker \\n 4 \\n  \\n \\n1.6 Roles and Responsibilities \\nRitesh Parmar \\n‚Ä¢ Role: Deep Learning Model Developer \\n‚Ä¢ Responsibilities: Designs and trains the vehicle detection model using \\nYOLOv8/MobileNet SSD architectures. Focuses on optimizing model \\nperformance for real-time detection accuracy and ensuring robust classification of \\nvehicles in no-vehicle areas. \\n      \\n            Pooja Parmar \\n‚Ä¢ Role: Web Backend & Database Engineer \\n‚Ä¢ Responsibilities: : Builds and maintains the Django -based backend \\nsystem, manages video uploads, data storage, and retrieval of detected \\nanomalies.. Ensures seamless communication between frontend, backend,  \\nand and database components for  smooth system functionality.. \\nRitika Mahajan \\n‚Ä¢ Role: Integration & Alerting Module Developer \\n‚Ä¢ Responsibilities: Develops the real -time alerting and anomaly detection \\nlogic that identifies vehicles entering restricted zones.  Integrates the AI \\nmodel output with the user interface and ensures accurate event logging \\nand visualization on the web dashboard.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 16, 'page_label': '17'}, page_content='Introduction \\nAI Vision Tracker \\n 5 \\n  \\n \\n1.7 Contribution of Project \\nThe project contributes to the field of intelligent surveillance by offering an \\nautomated solution for monitoring No Vehicle Zones. It replaces manual supervision \\nwith an AI-based system that detects and alerts whenever a vehicle enters restricted \\nareas. This helps improve pedestrian safety, supports traffic regulation, and promotes \\nsmarter urban management through real-time detection and alerts. \\n1.7.1 Market Potential \\no The system holds vast potential in both public and private sectors that require \\nreal-time enforcement of restricted zones. \\no Public Safety & Smart Cities: Can be integrated into smart surveillance \\nsystems to monitor pedestrian zones, marketplaces, and restricted roads. \\no Government & Law Enforcement: Useful for municipalities and traffic \\ndepartments to identify and penalize unauthorized vehicles.. \\no Educational Institutions: Helps schools and universities maintain safe \\ncampuses by preventing vehicle intrusions.. \\no Hospitals & Parks: Ensures vehicle-free, peaceful environments where only \\nemergency vehicles are allowed. \\nWith increasing adoption of AI in traffic and security management, such systems \\ncan play a crucial role in building safer, smarter urban spaces..'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 17, 'page_label': '18'}, page_content=\"Introduction \\nAI Vision Tracker \\n 6 \\n  \\n \\n1.7.2 Innovativeness \\no The proposed system introduces several innovative aspects that make it stand \\nout from traditional surveillance systems: \\no Dedicated Focus: Unlike general anomaly detection systems, it is designed \\nspecifically for No Vehicle Zone violations. \\no Region-Based Intelligence: Uses polygon-based zone mapping, allowing \\ndetection only within marked restricted areas to reduce false positives \\no Hybrid AI Pipeline: Combines object detection (for vehicles) and rule-based \\nlogic (for restricted zones) to achieve context-aware anomaly detection \\no Real-Time Alerts: Automatically generates alerts (visual/audio/email) when \\na vehicle intrusion is detected, ensuring instant response. \\no Modular Architecture: Each component ‚Äî detection, alerting, and data \\nmanagement ‚Äî can be independently scaled or upgraded. \\n1.7.3 Usefulness \\no The system provides practical value by enhancing users' ability to make \\ninformed fashion decisions: \\no Enhanced Safety: Protects pedestrians and reduces accident risks in vehicle-\\nrestricted areas. \\no Automation: Minimizes manual monitoring and human intervention, saving \\ntime and manpower.. \\no Evidence-Based Reporting: Provides video logs, timestamps, and detected \\nframes for verification and administrative action.. \\no Scalability: Can be extended to multiple zones or integrated with smart traffic \\nsystems for large-scale deployments. \\no Cost-Effective Solution: Utilizes existing camera infrastructure, reducing the \\nneed for new hardware.\"),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 18, 'page_label': '19'}, page_content='Introduction \\nAI Vision Tracker \\n 7 \\n  \\n \\n1.8 Report  Organization \\nThis report is systematically organized into several chapters to present a comprehensive \\nview of the project development process. Each chapter focuses on a specific stage of \\ndesigning and implementing the AI Vision Tracker ‚Äì No Vehicle Zone Anomaly \\nDetection System. The organization of the report ensures a smooth flow of information \\nfrom problem identification to implementation and final evaluation. \\nChapter 1: Introduction \\nThis chapter provides a detailed overview of the project, including its rationale, \\nobjectives, scope, and methodology. It introduces the motivation behind developing an \\nAI-based system for detecting vehicle anomalies in no-vehicle zones and outlines the \\nroles and contributions of each team member. \\nChapter 2: Literature Review \\nDiscusses the previous research and existing models in anomaly detection and computer \\nvision. It highlights the gaps in current surveillance systems and establishes the need for a \\nspecialized solution focusing on no-vehicle zones. \\nChapter 3: Software Requirement Specifications (SRS) \\nDefines the functional and non-functional requirements of the proposed system. It \\nexplains the software and hardware specifications, user interface design, and \\ncommunication requirements essential for implementation. \\nChapter 4: Methodology \\nDescribes the step-by-step process used in building the system. It includes data \\ncollection, preprocessing, dataset details, training and experimentation, the proposed \\nmodel, and the evaluation metrics used to assess performance.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 19, 'page_label': '20'}, page_content='Introduction \\nAI Vision Tracker \\n 8 \\n  \\n \\nChapter 5: Software Design Description (SDD) \\nProvides an in-depth description of the system‚Äôs architecture and design. It includes data \\nflow diagrams, use-case and class diagrams, database schema, architectural design, and \\ndetails of each functional module along with user interface design. \\nChapter 6: Software Test Documentation (STD) \\nCovers all testing procedures conducted during the development process. It includes the \\ntest plan, test cases, and methods used for unit, functional, system, integration, and \\nvalidation testing to ensure software reliability and accuracy. \\nChapter 7: Conclusion and Future Scope \\nSummarizes the achievements of the project, highlighting how it meets its objectives. It \\nalso outlines potential improvements and future enhancements, such as integration with \\nsmart city infrastructures and number plate recognition for automated enforcement.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 20, 'page_label': '21'}, page_content='Literature Review \\n \\nAI Vision Tracker  9  \\n \\nChapter-2 \\nLiterature Review  \\n  \\nThe literature review focuses on previous research conducted in the field of video \\nsurveillance, anomaly detection, and intelligent traffic monitoring . With the \\nadvancement of deep learning and computer vision, several models have been proposed \\nto automatically identify unusual activities or violations within video streams.  \\nWhile most existing systems address crowd behavior or general anomaly detection, \\nlimited work specifically targets vehicle intrusion detection in restricted or no -vehicle \\nzones. This chapter summarizes key studies that have contributed to the development of \\nintelligent monitoring systems using artificial intelligence. \\n \\nComparative Analysis of Existing Studies \\nTo understand the existing work in anomaly and vehicle detection, several research \\npapers were reviewed. \\nTable summarizes the key studies related to anomaly detection in surveillance systems \\nand their respective methods, contributions, and datasets. \\nMost existing models focus on crowd behavior analysis, human activity recognition, \\nor general surveillance anomalies, while only a limited number of studies explore \\nvehicle-based violations in restricted zones. \\nResearchers have experimented with various deep learning techniques such as CNNs, \\nAutoencoders, RNNs, and YOLO-based detectors for detecting unusual activities in \\nvideo frames. \\nAlthough these approaches have achieved good accuracy in general anomaly detection, \\nthey often lack context awareness ‚Äî i.e., the ability to distinguish between normal and \\nabnormal vehicle behavior in a specific no-vehicle region. \\nTherefore, this comparative study highlights the research gap that motivates the \\ndevelopment of the AI Vision Tracker, an AI-based solution designed specifically for \\ndetecting vehicle intrusions in restricted areas with real-time alerts and zone mapping.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 21, 'page_label': '22'}, page_content='Literature Review \\nAI Vision Tracker  10 \\n  \\n \\nAuthor and Year Method \\nD. Manju, Kishore K. \\nKumar  Movva Pavani  \\nmultistage pipeline combining MRCNN for multi-object \\nspatial detection and LSTM for temporal sequence analysis, \\ndesigned for precise early-stage anomaly classification in \\nsurveillance video. \\nGeorgescu et al, 2025 Self-Supervised Proxy Task Learning \\nJiang (first author), \\n2025 \\nHierarchical Spatio-Temporal Transformer U-Net \\n(HSTforU) \\nJiang & Mao, 2024 VLM-Assisted Unsupervised VAD \\nZhang (first author), \\n2024 \\nSpatio-Temporal Graph CNN + CRF \\nWenjie Peng, 2024 \\nWeakly supervised VAD with Spatio-Temporal Prompts \\n(VLM) \\nAbhinav Kumar Rai, \\n2024 \\nSpatio-Temporal Pseudo-Anomaly Generation \\n(Unsupervised, OCC) \\nJunghyun Lee, 2023 Cognitive Refined Augmentation (Weakly supervised) \\nYifan Pu, 2023 Prompt-Enhanced Context Features (Weakly supervised) \\nHaosen Lv, 2023 Unbiased Multiple Instance Learning (Weakly supervised) \\nTable 1: Literature Review'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 22, 'page_label': '23'}, page_content='Literature Review \\nAI Vision Tracker  11 \\n  \\n \\nThe table presents a comparative summary of major research studies conducted in the \\nfield of video anomaly detection and intelligent surveillance. It outlines the authors,  and \\nmethods used employed in each study. \\nFrom the table, it is observed that most researchers have focused on identifying abnormal \\nactivities in public or crowded spaces using deep learning models such as CNNs, \\nAutoencoders, and 3D-CNNs. \\n \\nWhile these approaches show promising results for general anomaly detection, very few \\nstudies emphasize vehicle-based anomalies, especially in restricted or no-vehicle zones. \\nThe comparison highlights that existing methods often lack region-specific detection \\nlogic, which limits their applicability in real-time traffic control or pedestrian-only \\nenvironments. \\nThis observation reinforces the need for the proposed AI Vision Tracker, which \\nuniquely integrates object detection, zone mapping, and alert generation for detecting \\nunauthorized vehicle entries in real-world no-vehicle areas.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 23, 'page_label': '24'}, page_content='Software Requirement Specifications \\n \\nAI Vision Tracker  \\n 12 \\nChapter-3 \\nSoftware Requirement Specifications (SRS)  \\n3.1 Introduction \\nThe Software Requirement Specification (SRS) defines the complete set of functional \\nand non-functional requirements for the proposed system ‚ÄúAI Vision Tracker ‚Äì No \\nVehicle Zone Anomaly Detection System.‚Äù \\nThis document serves as a blueprint for the design, development, and implementation of \\nthe project. It specifies the features, system behavior, constraints, and interface \\nrequirements to ensure the final product meets user expectations. \\nThe system aims to automatically detect unauthorized vehicle movement in restricted \\nzones using AI-powered video analytics. It minimizes human intervention and provides \\nreal-time alerts, logs, and data visualization through a web-based interface. \\n \\n 3.1.1 Product Overview \\nThe proposed system is an AI-based anomaly detection solution  that uses computer \\nvision and deep learning to identify vehicles in No Vehicle Zones . It continuously \\nmonitors CCTV or IP camera feeds, detects vehicles entering restricted areas, and \\nimmediately triggers an alert while logging the event with a timestamp on the admin \\ndashboard. \\nKey components include: \\n‚Ä¢ Video Input Module: Captures real-time or recorded video streams. \\n‚Ä¢ Detection Engine: Identifies vehicles using trained AI models. \\n‚Ä¢ Zone Mapping Module: Defines and monitors restricted regions. \\n‚Ä¢ Alert & Logging Module: Generates alerts and stores event data. \\n‚Ä¢ Web Dashboard: Displays real-time detections and reports.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 24, 'page_label': '25'}, page_content='Software Requirement Specifications \\n \\nAI Vision Tracker                                                                                                                                                       \\n 13 \\n \\n3.2 Software Functional Requirements \\nThe functional requirements define the key operations and expected behavior of the \\nsystem. \\n‚Ä¢ Vehicle Detection: \\nDetects vehicles in real-time from CCTV or IP camera feeds using the trained AI \\nmodel and differentiates them from non-vehicle objects. \\n‚Ä¢ Zone Violation Monitoring: \\nContinuously monitors defined restricted areas and detects when a vehicle enters \\na No Vehicle Zone. \\n‚Ä¢ Alert Generation: \\nInstantly triggers alerts such as on-screen warnings, audio notifications, or emails \\nupon detecting a violation. \\n‚Ä¢ Event Logging and Storage: \\nRecords every detected event with its timestamp, image, and video snippet in the \\ndatabase for analysis. \\n‚Ä¢ Web-Based Interface: \\nProvides an intuitive dashboard for uploading videos, marking restricted zones, \\nand reviewing anomaly reports. \\n3.2.1 Distributed Database or Client‚ÄìServer Model \\nThe system follows a client‚Äìserver architecture where the client handles video input \\nand visualization, while the server processes detection, storage, and alerts. \\n‚Ä¢ The Client side (frontend) runs on a web browser, displaying camera feeds and \\nalert dashboards. \\n‚Ä¢ The Server side (backend) hosts the trained model, processes video frames, and \\nmanages data in the database.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 25, 'page_label': '26'}, page_content='Software Requirement Specifications \\n \\nAI Vision Tracker                                                                                                                                                       \\n 14 \\n‚Ä¢ A Distributed database ensures data consistency and scalability, allowing \\nmultiple clients to connect simultaneously that supports high performance. \\n3.3  Non- Functional Requirement \\nNon-functional requirements define the quality attributes of the system, describing how \\nthe software performs rather than what it does. These ensure that the system remains \\nstable, secure, efficient, and easy to maintain during operation. \\nFor the AI Vision Tracker ‚Äì No Vehicle Zone Anomaly Detection System, non-\\nfunctional parameters such as reliability, availability, security, maintainability, \\nportability, and performance are critical for ensuring consistent and real-time surveillance \\nin sensitive environments. \\n \\n3.3.1 Reliability \\nReliability ensures the system performs accurately and consistently under varying \\nweather, lighting, and camera conditions. The trained AI model minimizes false \\ndetections and missed anomalies, ensuring dependable performance. \\nAutomated monitoring and error-logging mechanisms maintain stability, while the \\nsystem automatically resumes operation after network or hardware interruptions. Regular \\nretraining and feedback improve long-term accuracy and reliability. \\nThe system is also stress-tested with multiple camera feeds to ensure steady performance \\nunder heavy load. Continuous evaluation of detection accuracy helps maintain consistent \\nresults over time. \\n3.3.2 Availability \\nAvailability ensures the system remains active and accessible for continuous surveillance \\nwith minimal downtime. Designed for 24√ó7 operation, it automatically restarts and \\nrecovers after network or hardware interruptions. \\nRegular maintenance and data backup improve uptime, while the web dashboard stays'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 26, 'page_label': '27'}, page_content='Software Requirement Specifications \\n \\nAI Vision Tracker                                                                                                                                                       \\n 15 \\nconsistently accessible to authorized users to ensure no intrusion goes unnoticed. The \\nweb dashboard must accessible to authorized users at all times. \\n3.3.3 Security \\nSecurity is a critical aspect of the system as it processes live surveillance data and \\nsensitive event logs. All communication between the client and server is encrypted using \\nSSL/TLS protocols to prevent unauthorized interception or data breaches. \\nThe system includes user authentication and role-based access control (RBAC) to \\nensure only authorized personnel can access or modify data. Activity logs are maintained \\nfor every user operation to track any unauthorized attempt or anomaly. \\nDatabase encryption, secure file storage, and regular backups protect video evidence and \\nreports from tampering or deletion. The system also incorporates firewall protection and \\nsecure API endpoints to strengthen overall data integrity and security. \\n3.3.4 Maintainability \\nMaintainability ensures that the system can be updated, debugged, or modified easily \\nover time without affecting its stability. The project follows a modular and layered \\narchitecture, allowing individual components‚Äîsuch as detection, database, and alert \\nmodules‚Äîto be modified independently. \\nComprehensive documentation, code commenting, and proper naming conventions \\nimprove readability and collaboration among developers. \\nVersion control tools like Git are used for tracking code changes, while testing and error \\nlogging help identify and fix issues quickly. \\nThis design ensures long-term sustainability, enabling smooth integration of newer AI \\nmodels, improved algorithms, or hardware upgrades with minimal downtime.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 27, 'page_label': '28'}, page_content='Software Requirement Specifications \\n \\nAI Vision Tracker                                                                                                                                                       \\n 16 \\n \\n3.3.5 Portability \\nPortability defines how easily the system can operate across different hardware and \\nsoftware environments. The proposed AI Vision Tracker is designed to run seamlessly on \\nWindows, Linux, and cloud-based platforms with minimal configuration changes. \\nIt supports integration with various CCTV, IP, and USB cameras, allowing flexible \\ndeployment in schools, hospitals, marketplaces, or smart city zones. \\nThe system‚Äôs web-based architecture and use of cross-platform tools ensure smooth \\nmigration between local servers and cloud infrastructures. This adaptability makes the \\nsolution scalable and cost-effective for diverse organizations. \\nMinimal setup requirements and dependency management enable quick installation and \\nexecution across multiple environments without significant code modifications. \\n3.3.6 Performance \\nPerformance ensures that the system processes video streams efficiently and provides \\nreal-time detection with minimal delay. The AI model is optimized to achieve at least 15‚Äì\\n20 frames per second (FPS), ensuring instant response to vehicle intrusions. \\nThe use of GPU acceleration, multi-threading, and optimized data pipelines enhances \\nframe processing speed and reduces latency. \\nThe database and server modules are designed for fast data retrieval and smooth alert \\ndelivery even under heavy workloads or multiple camera connections. \\nRegular performance testing, including response time and throughput analysis, ensures \\nthat the system remains stable, accurate, and responsive during continuous surveillance \\noperations.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 28, 'page_label': '29'}, page_content='Software Requirement Specifications \\n \\nAI Vision Tracker                                                                                                                                                       \\n 17 \\n3.4 External Interface Requirements \\nThe external interface requirements describe how the system interacts with users, \\nhardware devices, software modules, and communication networks. These interfaces \\nensure that the system‚Äôs components function cohesively and provide a smooth \\nexperience for operators and administrators. \\n3.4.1 User Interface \\nThe user interface is a web-based dashboard that allows users to view live camera \\nfeeds, define restricted zones, and monitor detected anomalies in real time. \\nIt includes modules for video upload, zone marking, alert management, and report \\ngeneration, each designed for quick access and usability. \\nDeveloped using modern frameworks like React or Django Templates, the interface is \\nresponsive, lightweight, and compatible across desktops and mobile devices. Its clean \\nlayout ensures smooth operation for both technical and non-technical users.. \\n3.4.2 Hardware Interface \\nThe system is compatible with a wide range of CCTV, IP, and USB cameras for video \\ninput. It can operate using a GPU-enabled system or cloud-based processing \\nenvironment to handle real-time object detection efficiently. \\nOther required hardware includes: \\n‚Ä¢ A standard computing device or server with high processing power (for \\ninference). \\n‚Ä¢ Sufficient RAM and storage to manage video data and event logs. \\n‚Ä¢ External alerting devices such as speakers or alarms for real-time notifications. \\nThe modular hardware setup ensures flexibility ‚Äî additional cameras or storage \\nunits can be added without affecting system performance.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 29, 'page_label': '30'}, page_content='Software Requirement Specifications \\n \\nAI Vision Tracker                                                                                                                                                       \\n 18 \\n3.4.3 Software Interface \\nThe proposed system interacts with multiple software components to perform detection, \\nstorage, and reporting tasks. \\n‚Ä¢ Frontend: Built using React.js/HTML/CSS for the interactive user dashboard. \\n‚Ä¢ Backend: Developed with Django or Flask to handle data processing, API \\nrequests, and model execution. \\n‚Ä¢ Database: Uses MySQL or PostgreSQL to store event logs, user data, and \\nsystem configurations. \\n‚Ä¢ AI Frameworks: Employs TensorFlow, PyTorch, and OpenCV for image \\nrecognition and anomaly detection. \\n‚Ä¢ Operating Systems Supported: Windows 10/11, Ubuntu 20.04+, or cloud \\nservers like AWS and Google Cloud. \\nThe interaction between these software modules is handled using RESTful APIs \\nfor smooth and reliable communication. \\n3.4.4 Communication Interface \\nCommunication between system components occurs through secure RESTful APIs over \\nHTTP/HTTPS protocols. Data such as detection results, alerts, and logs are transmitted in \\nreal time between the client (UI) and server. \\nThe system supports WebSocket connections for live video streaming and instant alert \\ndelivery. \\nEncryption through SSL/TLS ensures secure data transfer between devices. \\nOptional communication extensions include email or SMS notifications for alert \\nescalation, enabling administrators to receive real-time updates on violations even when \\naway from the control dashboard. \\nThis interface design ensures continuous, reliable, and secure connectivity between all \\nactive components of the system.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 30, 'page_label': '31'}, page_content='Methodology \\n \\nAI Vision Tracker \\n 19  \\n \\nChapter-4 \\nMethodology  \\n \\n4.1 Data Collection  \\nThe data collection process forms the foundation of the system‚Äôs model training. \\nVideo footage and image datasets were collected from various publicly available \\nsources and custom-recorded CCTV feeds representing real-world ‚Äúno-vehicle‚Äù \\nenvironments such as parks, school campuses, and marketplaces. \\nThe dataset includes a balanced mix of normal scenarios (pedestrian-only areas) and \\nanomalous events (vehicles entering restricted zones). \\nData diversity ensures the model‚Äôs ability to generalize across different lighting \\nconditions, camera angles, and environmental settings. \\n \\n4.2 Data Preprocessing \\nBefore training, all video data is converted into image frames and preprocessed to \\nenhance detection accuracy. \\nKey preprocessing steps include: \\n‚Ä¢ Frame Extraction: Videos are split into continuous frames for individual image \\nanalysis. \\n‚Ä¢ Resizing and Normalization: Images are resized (e.g., 640√ó640) and normalized \\nto standardize input data for the model. \\n‚Ä¢ Annotation: Frames containing vehicles are manually labeled using bounding \\nboxes to create accurate ground truth data. \\n‚Ä¢ Noise Reduction: Irrelevant or blurry frames are removed to improve dataset \\nquality. \\n‚Ä¢ Data Augmentation: Techniques like flipping, rotation, and brightness \\nadjustment increase dataset variety and reduce overfitting.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 31, 'page_label': '32'}, page_content='Methodology \\nAI Vision Tracker \\n 20 \\n  \\n \\n4.3 Dataset Details \\nThe dataset used in this project is a combination of custom-collected camera footage \\nand open-source datasets such as UA-DETRAC, COCO Vehicle Subset, and Stanford \\nCars. \\nEach image is annotated with object labels like car, bike, truck, etc. \\nThe dataset is divided into three subsets: \\n‚Ä¢ Training Set (70%) ‚Äì Used to train the model. \\n‚Ä¢ Validation Set (20%) ‚Äì Used to tune hyperparameters and evaluate model \\ngeneralization. \\n‚Ä¢ Testing Set (10%) ‚Äì Used for final performance evaluation. \\nThis diverse and well-balanced dataset ensures that the model performs accurately \\nin both controlled and real-world conditions. \\n \\n4.4 Training & Experimentation on Dataset \\nThe model was trained using deep learning frameworks such as TensorFlow and \\nPyTorch, implementing object detection architectures like YOLOv8 or MobileNet SSD. \\nTraining was performed on GPU-enabled systems to achieve real-time inference \\ncapability. \\nKey steps during training included: \\n‚Ä¢ Defining anchor boxes and bounding box regression for vehicle localization. \\n‚Ä¢ Applying transfer learning to fine-tune pre-trained models on custom datasets. \\n‚Ä¢ Using loss functions (like cross-entropy and IoU loss) for optimization. \\n‚Ä¢ Running multiple epochs with gradually adjusted learning rates for stability. \\nThe experimentation phase involved testing different configurations of batch size, \\nlearning rate, and dataset splits to find the optimal model performance.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 32, 'page_label': '33'}, page_content='Methodology \\nAI Vision Tracker \\n 21 \\n  \\n \\n4.5 Block Diagram of Methodology \\nThe overall workflow of the system is illustrated in the block diagram below (in the \\nreport figure section): \\nDescription: \\n1. The system captures live video input or pre-recorded footage. \\n2. Frames are processed by the trained AI model for vehicle detection. \\n3. The detected objects are checked against predefined no-vehicle zones. \\n4. If a vehicle is found inside a restricted zone, an alert is generated and logged. \\n5. The detected events and alerts are displayed on the web dashboard for monitoring. \\n \\n \\n \\nFigure 1: Block Diagram'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 33, 'page_label': '34'}, page_content='Methodology \\nAI Vision Tracker \\n 22 \\n  \\n \\n4.6 Details of Proposed Model for Implementation \\nThe proposed model integrates object detection and zone-based logic for context-aware \\nanomaly detection. \\n‚Ä¢ Model Architecture: YOLOv8 / MobileNet SSD used for high-speed detection \\nwith minimal computational load. \\n‚Ä¢ Input Processing: Each frame is passed through convolutional layers to extract \\nspatial features. \\n‚Ä¢ Detection Logic: The model identifies vehicle classes and compares object \\ncoordinates with restricted zone boundaries using polygon mapping. \\n‚Ä¢ Alert System: When a vehicle enters a restricted area, an alert is triggered and \\nstored in the database with timestamps. \\n‚Ä¢ Integration: The AI module communicates with the backend (Django/Flask) to \\nupdate the dashboard in real time. \\n4.7 Performance Evaluation Measures \\nTo validate system efficiency, several performance metrics are used: \\n‚Ä¢ Accuracy: Measures correct detections versus total detections. \\n‚Ä¢ Precision & Recall: Evaluate the model‚Äôs ability to correctly identify vehicle \\nintrusions and avoid false alerts. \\n‚Ä¢ F1-Score: Represents the harmonic mean of precision and recall, giving a \\nbalanced evaluation metric. \\n‚Ä¢ IoU (Intersection over Union): Measures how closely the predicted bounding \\nbox matches the ground truth. \\n‚Ä¢ FPS (Frames Per Second): Determines real-time performance capability. \\nTesting showed consistent results with high precision and low false detection \\nrates, proving the system‚Äôs effectiveness in real-world no-vehicle scenarios.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 34, 'page_label': '35'}, page_content='Software Design Description \\n \\nAI Vision Tracker  23   \\n \\nChapter-5 \\nSoftware Design Description  \\n \\n5.1 Introduction \\nThe Software Design Description (SDD) provides a comprehensive overview of the \\ndesign and structure of the AI Vision Tracker system ‚Äî an AI-powered surveillance \\nsolution for detecting vehicle intrusions within No Vehicle Zones. This chapter \\nexplains how various software components, modules, and database entities interact to \\nachieve the system‚Äôs objectives. The design focuses on modularity, scalability, and \\nreal-time processing to ensure accurate anomaly detection and alert generation. \\n5.2 Design Overview \\nThe overall design of the system emphasizes the flow of data from input sources (CCTV \\nor drone cameras) through detection models, anomaly identification modules, and finally \\nto alert generation and storage systems. The architecture is layered into distinct functional \\nunits such as data acquisition, AI processing, database management, and user interface. \\n5.2.1 Data Flow Diagram \\nThe Data Flow Diagram (DFD) illustrates how data moves through the system ‚Äî from \\nvideo capture to detection, anomaly identification, and alert notification. \\nKey processes include: \\n‚Ä¢ Capturing live video from cameras \\n‚Ä¢ Detecting and classifying vehicles using deep learning models \\n‚Ä¢ Checking zone boundaries for restricted entry \\n‚Ä¢ Generating alerts for detected anomalies \\n‚Ä¢ Logging all activities in the database'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 35, 'page_label': '36'}, page_content='Software Design Description \\nAI Vision Tracker  24 \\n  \\n \\n \\nFigure 2 : Level 0 DFD \\n \\n \\nFigure 3: Level 1 DFD'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 36, 'page_label': '37'}, page_content='Software Design Description \\nAI Vision Tracker  25 \\n  \\n \\n5.2.2 Use-Case Diagram \\nThe Use-Case Diagram identifies the main actors and their interactions with the \\nsystem.  \\nMain Use Cases: \\n1. Configure camera and zone details \\n2. Detect vehicles in real-time \\n3. Identify anomalies in restricted zones \\n4. Generate and send alerts \\n5. View historical event logs and analytics \\n \\n \\nFigure 4 : Use case Diagram'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 37, 'page_label': '38'}, page_content='Software Design Description \\nAI Vision Tracker  26 \\n  \\n \\n5.2.3 Class Diagram \\nThe Class Diagram defines the main software classes and their relationships. It \\nprovides a blueprint for implementing object-oriented structures. \\nKey Classes: \\n‚Ä¢ Camera ‚Äì attributes like camera ID, location, and zone reference \\n‚Ä¢ Detection ‚Äì manages detection events and classification results \\n‚Ä¢ Anomaly ‚Äì represents abnormal detections such as vehicles in restricted zones \\n‚Ä¢ Alert ‚Äì handles alert messages and statuses \\n‚Ä¢ User ‚Äì represents administrators and operators \\n‚Ä¢ Dashboard‚Äì stores user and system activity'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 38, 'page_label': '39'}, page_content='Software Design Description \\nAI Vision Tracker  27 \\n  \\n \\n \\nFigure 5 : Class Diagram'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 39, 'page_label': '40'}, page_content='Software Design Description \\nAI Vision Tracker  28 \\n  \\n \\n5.3 Database Design \\nThe database design ensures efficient storage, retrieval, and management of all \\ndetection and alert data. It is structured to support real-time operations with referential \\nintegrity and minimal redundancy. \\n \\n5.3.1 Entity Relationship Diagram (ERD) \\nThe ERD depicts the logical relationships between system entities such as Zone, \\nCamera, Detection, Anomaly, Alert, User, and EventLog. \\nKey Relationships: \\n‚Ä¢ One Zone ‚Üí Multiple Cameras \\n‚Ä¢ One Camera ‚Üí Multiple Detections \\n‚Ä¢ One Detection ‚Üí One Anomaly \\n‚Ä¢ One Anomaly ‚Üí One Alert \\n‚Ä¢ One User ‚Üí Multiple Event Logs'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 40, 'page_label': '41'}, page_content='Software Design Description \\nAI Vision Tracker  29 \\n  \\n \\n Figure 6: Entity Relationship Diagram (ERD) \\n \\n \\n5.3.2 Normalization Details \\nThe database is normalized up to the Third Normal Form (3NF)  to ensure data \\nconsistency and eliminate redundancy. \\nNormalization Process: \\n‚Ä¢ 1NF: All tables have atomic values with unique records. \\n‚Ä¢ 2NF: Non-key attributes depend entirely on the primary key. \\n‚Ä¢ 3NF: No transitive dependencies exist between non-key attributes. \\nThis ensures optimal database performance and maintainability. \\n \\n5.3.3 Table Structure'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 41, 'page_label': '42'}, page_content='Software Design Description \\nAI Vision Tracker  30 \\n  \\n \\nThe database comprises seven key tables: Zone, Camera, Detection, Anomaly, Alert, \\nUser, and EventLog. \\nEach table defines specific entities and relationships used throughout the system for \\nanomaly tracking and alerting. \\n \\nFigure 7: Table Structure \\n \\n5.3.4 Data Dictionary \\nThe Data Dictionary defines each table‚Äôs attributes, data types, and relationships. It \\nprovides developers and database administrators with clear references for \\nimplementation.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 42, 'page_label': '43'}, page_content=\"Software Design Description \\nAI Vision Tracker  31 \\n  \\n \\nEach table ‚Äî such as Zone, Camera, Detection, Anomaly, Alert, User, and EventLog ‚Äî \\nhas defined primary keys, foreign keys, and data types that maintain data consistency and \\nsupport system functionality. \\n \\n    Table 2: Zone \\nField Name Data Type Constraint Description \\nzone_id INT Primary Key, Auto \\nIncrement \\nUnique identifier for \\neach restricted zone. \\nzone_name VARCHAR(100) NOT NULL \\nName of the restricted \\narea (e.g., ‚ÄúSchool \\nEntrance‚Äù). \\nlocation_details VARCHAR(255) NULL Additional details about \\nzone location. \\ncoordinates JSON NOT NULL \\nPolygon coordinates \\nrepresenting the No \\nVehicle Zone area. \\ncreated_at TIMESTAMP DEFAULT \\nCURRENT_TIMESTAMP \\nTimestamp when the \\nzone was registered. \\n \\nTable 3: Camera \\nField \\nName Data Type Constraint Description \\ncamera_id INT Primary Key, Auto Increment Unique identifier for each \\ncamera. \\nlocation VARCHAR(255) NOT NULL Physical location where \\nthe camera is installed. \\nzone_id INT Foreign Key \\nLinks the camera to the \\nrespective zone it \\nmonitors. \\nstatus VARCHAR(50) DEFAULT 'Active' Operational status of the\"),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 43, 'page_label': '44'}, page_content=\"Software Design Description \\nAI Vision Tracker  32 \\n  \\n \\nField \\nName Data Type Constraint Description \\ncamera (Active/Inactive). \\ninstalled_at TIMESTAMP DEFAULT \\nCURRENT_TIMESTAMP \\nInstallation timestamp of \\nthe camera. \\n \\n \\nTable 4: Detection \\nField Name Data Type Constraint Description \\ndetection_id INT Primary Key, Auto Increment Unique identifier for \\neach detection event. \\ncamera_id INT Foreign Key The camera that \\ncaptured the detection. \\ndetected_object VARCHAR(50) NULL \\nObject class detected \\n(e.g., ‚ÄúCar‚Äù, ‚ÄúBike‚Äù, \\n‚ÄúPerson‚Äù). \\nconfidence_level FLOAT NULL Confidence score from \\nthe AI model. \\ndetection_time TIMESTAMP DEFAULT \\nCURRENT_TIMESTAMP \\nTime when the \\ndetection occurred. \\n \\n \\nTable 5: Anomaly \\nField Name Data Type Constraint Description \\nanomaly_id INT Primary Key, Auto Increment Unique identifier for \\neach anomaly. \\ndetection_id INT Foreign Key Links the anomaly to \\nits original detection. \\nanomaly_type VARCHAR(100) DEFAULT 'Vehicle Entry' Type of anomaly\"),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 44, 'page_label': '45'}, page_content=\"Software Design Description \\nAI Vision Tracker  33 \\n  \\n \\nField Name Data Type Constraint Description \\ndetected. \\nsnapshot_path VARCHAR(255) NULL File path of image \\nsnapshot from video. \\ntimestamp TIMESTAMP DEFAULT \\nCURRENT_TIMESTAMP \\nTimestamp of the \\nanomaly event. \\n \\n \\nTable 6: Alert \\nField \\nName Data Type Constraint Description \\nalert_id INT Primary Key, Auto Increment Unique identifier for each \\nalert sent. \\nanomaly_id INT Foreign Key The anomaly that \\ntriggered the alert. \\nalert_type VARCHAR(50) DEFAULT 'Email' Type of alert generated \\n(Email, SMS, Dashboard). \\nalert_status VARCHAR(50) DEFAULT 'Sent' Current status of the alert \\n(Sent, Pending, Failed). \\nsent_time TIMESTAMP DEFAULT \\nCURRENT_TIMESTAMP Time when alert was sent. \\n \\n \\nTable 7: User \\nField Name Data Type Constraint Description \\nuser_id INT Primary Key, Auto Increment Unique identifier for \\neach user. \\nname VARCHAR(100) NOT NULL Full name of the \\nsystem user.\"),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 45, 'page_label': '46'}, page_content=\"Software Design Description \\nAI Vision Tracker  34 \\n  \\n \\nField Name Data Type Constraint Description \\nrole VARCHAR(50) CHECK (role IN ('Admin', \\n'Security', 'Operator')) \\nRole or access level \\nin the system. \\nemail VARCHAR(150) NULL \\nEmail address for \\ncommunication or \\nlogin. \\npassword_hash VARCHAR(255) NULL Encrypted password \\nfor authentication. \\ncreated_at TIMESTAMP DEFAULT \\nCURRENT_TIMESTAMP \\nAccount creation \\ntimestamp. \\n \\nTable 8: EventLog \\nField Name Data Type Constraint Description \\nevent_id INT Primary Key, Auto Increment Unique identifier for \\neach logged event. \\nuser_id INT Foreign Key The user who performed \\nthe action. \\nevent_description TEXT NULL \\nDescription of the event \\n(e.g., ‚ÄúAlert sent‚Äù, \\n‚ÄúCamera added‚Äù). \\nevent_time TIMESTAMP DEFAULT \\nCURRENT_TIMESTAMP \\nTimestamp of when the \\nevent occurred. \\n \\n \\n5.4 Data Flow Diagram \\nThis section provides a detailed depiction of how data flows within the system from start \\nto end. \\n‚Ä¢ Input: Video feed captured from camera \\n‚Ä¢ Processing: Vehicle detection via AI model (YOLOv8/MobileNet SSD) \\n‚Ä¢ Verification: Zone validation for No Vehicle Zone\"),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 46, 'page_label': '47'}, page_content='Software Design Description \\nAI Vision Tracker  35 \\n  \\n \\n‚Ä¢ Output: Alert generation and database storage \\n \\n \\nFigure 8 : Level 0 DFD \\n \\n5.5 System Architectural Design \\nThe System Architectural Design  defines the high -level structure of the AI Vision \\nTracker system, describing how its major components interact to achieve real -time \\nvehicle anomaly detection in No Vehicle Zones.  \\nThe architecture follows a modular, layered, and scalable design , ensuring efficient \\nprocessing, easy maintenance, and adaptability to new environments or models. \\nArchitectural Overview \\nThe system is divided into five primary layers , each responsible for a specific set of \\ntasks: \\n1. Input Layer (Data Acquisition): \\no Captures live or recorded video streams from fixed CCTV or drone -based \\ncameras. \\no Uses the OpenCV library for frame extraction, resizing, and noise \\nreduction. \\no Ensures consistent video input format for downstream AI processing. \\n2. Processing Layer (AI & Detection Module): \\no Utilizes pre-trained object detection models like YOLOv8 or MobileNet \\nSSD. \\no Detects moving objects and classifies them as vehicles or pedestrians.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 47, 'page_label': '48'}, page_content='Software Design Description \\nAI Vision Tracker  36 \\n  \\n \\no Implements region -based filtering to identify whether a detected vehicle \\nfalls within a restricted (No Vehicle) zone. \\n3. Anomaly Analysis & Decision Layer: \\no Compares detection outputs with predefined zone boundaries. \\no Flags violations (e.g., vehicles entering pedestrian zones). \\no Assigns severity levels and generates structured event data for further \\nactions. \\n4. Alert & Notification Layer: \\no Automatically triggers alerts in real time upon detecting anomalies. \\no Supports multiple alert mechanisms such as visual signals, emails, or \\ndashboard notifications. \\no Maintains an alert log for tracking and reviewing previous incidents. \\n \\n5. Storage & Monitoring Layer (Database + Dashboard): \\no Stores detections, anomalies, and alert details in a relational database. \\no Provides an intuitive web-based dashboard  for administrators and \\nsecurity staff to: \\n‚ñ™ Review event histories \\n‚ñ™ Monitor live feeds \\n‚ñ™ Manage users, cameras, and restricted zones'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 48, 'page_label': '49'}, page_content='Software Design Description \\nAI Vision Tracker  37 \\n  \\n \\n \\nFigure 9: System Architectural \\n \\n5.6 Detailed Description of Components (Modules) \\nThis section provides a detailed explanation of each functional module in the AI-Powered \\nVehicle Detection System for No-Vehicle Zones. Each module performs a specific role in \\nthe overall workflow, ensuring seamless data flow, accurate vehicle detection, and timely \\nalert generation. \\n \\n5.6.1 Module 1 ‚Äì Input Sources Module \\nPurpose: \\nThis module serves as the entry point of the system, handling different types of video \\ninputs such as live camera feeds or pre-recorded video files. It ensures that incoming data \\nis appropriately formatted and standardized for further processing. \\nFunctions: \\n‚Ä¢ Capture video streams from live CCTV cameras. \\n‚Ä¢ Accept uploaded video files for analysis. \\n‚Ä¢ Extract individual frames from the video. \\n‚Ä¢ Perform resizing and normalization to maintain consistent input resolution and \\nquality.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 49, 'page_label': '50'}, page_content='Software Design Description \\nAI Vision Tracker  38 \\n  \\n \\nInput: \\n‚Ä¢ Live video feed / uploaded video file. \\nOutput: \\n‚Ä¢ Preprocessed and normalized video frames. \\nTechnology Used: \\n‚Ä¢ OpenCV, FFmpeg, Python. \\n \\n5.6.2 Module 2 ‚Äì Video Input & Preprocessing Module \\nPurpose: \\nThis module performs the core computer vision tasks required to detect vehicles in the \\nvideo frames. It extracts spatial and temporal features that represent vehicle presence and \\nmovement behavior. \\nFunctions: \\n‚Ä¢ Apply object detection models (YOLOv5 / MobileNet -SSD / YOLOS) to identify \\nvehicles in each frame. \\n‚Ä¢ Extract spatial features (object boundaries, positions, and confidence scores). \\n‚Ä¢ Use temporal learners (RNN / LSTM) to analyze motion patterns across frames. \\n‚Ä¢ Generate a behavioral score that reflects the detected vehicle‚Äôs activity within the \\nrestricted zone. \\nInput: \\n‚Ä¢ Preprocessed video frames. \\nOutput: \\n‚Ä¢ Feature-extracted data with behavioral scores. \\nTechnology Used: \\n‚Ä¢ TensorFlow / PyTorch, OpenCV, NumPy, Scikit-learn.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 50, 'page_label': '51'}, page_content='Software Design Description \\nAI Vision Tracker  39 \\n  \\n \\n5.6.3 Module 3 ‚Äì Anomaly Flagging & Alerting Module \\nPurpose: \\nThis module identifies and flags any detected vehicle activity that violates the No -\\nVehicle Zone policy. It then generates automated alerts for authorities or system \\nadministrators. \\nFunctions: \\n‚Ä¢ Apply threshold logic to the behavioral score for anomaly detection. \\n‚Ä¢ Log each anomaly event in the database. \\n‚Ä¢ Trigger email or SMS alerts to the concerned authorities. \\n‚Ä¢ Maintain a record of historical anomaly data for analysis. \\nInput: \\n‚Ä¢ Behavioral score and detection results from the preprocessing module. \\nOutput: \\n‚Ä¢ Alert notifications and stored anomaly logs. \\nTechnology Used: \\n‚Ä¢ Python (smtplib), Database API, Alerting Services. \\n \\n5.6.4 Module 4 ‚Äì Data Storage & Web Panel Module \\nPurpose: \\nThis module provides the administrative interface and storage backend for the entire \\nsystem. It ensures that all processed data, anomalies, and alerts are securely stored and \\neasily accessible through an interactive dashboard. \\nFunctions: \\n‚Ä¢ Store detected anomalies, logs, and system statistics. \\n‚Ä¢ Provide a web-based dashboard (Django / Flask / PyQt) for visualization.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 51, 'page_label': '52'}, page_content='Software Design Description \\nAI Vision Tracker  40 \\n  \\n \\n‚Ä¢ Allow administrators to monitor live feeds and analyze historical trends. \\n‚Ä¢ Manage user roles, system configurations, and report generation. \\nInput: \\n‚Ä¢ Anomaly logs, detection events, and system data. \\nOutput: \\n‚Ä¢ Visual reports, dashboards, and stored datasets. \\nTechnology Used: \\n‚Ä¢ Django / Flask, MySQL / PostgreSQL, HTML, CSS, JavaScript. \\n5.7 User Interface Design \\nThe user interface (UI) of the AI-Powered Surveillance System for No Vehicle Zone \\nDetection has been designed to be intuitive, user-friendly, and responsive, ensuring \\nsmooth interaction between the end user and the system. \\nIt provides a centralized dashboard where administrators can monitor live video streams, \\nview detected anomalies, and manage alert notifications efficiently. \\n \\n5.7.1 Description of the User Interface \\nThe system interface is divided into several key panels, each with a specific purpose: \\n1. Login Page \\n‚Ä¢ Allows authorized users (e.g., administrators, security officers) to securely access \\nthe system. \\n‚Ä¢ Includes user authentication with username and password. \\n‚Ä¢ Provides password reset or help options for convenience.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 52, 'page_label': '53'}, page_content='Software Design Description \\nAI Vision Tracker  41 \\n  \\n \\n2. Dashboard Panel \\n‚Ä¢ Acts as the central monitoring hub. \\n‚Ä¢ Displays live video feeds from connected CCTV cameras. \\n‚Ä¢ Shows real-time alerts whenever a vehicle enters a restricted zone. \\n‚Ä¢ Contains system statistics such as total alerts, active cameras, and zone \\nconfigurations. \\n \\n3. Alerts and Events Log Panel \\n‚Ä¢ Lists all detected anomalies with timestamps, camera ID, and vehicle type. \\n‚Ä¢ Allows users to filter and sort alerts based on date, time, or location. \\n‚Ä¢ Provides access to video snapshots and short clips of the event for quick review. \\n \\n4. Reports & Analytics Panel \\n‚Ä¢ Provides graphical insights into historical anomaly trends (daily, weekly, or \\nmonthly). \\n‚Ä¢ Displays performance metrics and system activity summaries. \\n‚Ä¢ Allows export of reports in PDF or Excel format. \\n \\n5. Settings Panel \\n‚Ä¢ Used for user management, camera connection setup, and alert preferences \\n(email, SMS, dashboard notifications). \\n‚Ä¢ Provides system maintenance controls such as database backup and log clearing.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 53, 'page_label': '54'}, page_content='Software Design Description \\nAI Vision Tracker  42 \\n  \\n \\n5.7.2 Screen Images \\n‚Ä¢ Home page'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 54, 'page_label': '55'}, page_content='Software Design Description \\nAI Vision Tracker  43 \\n  \\n \\n‚Ä¢ Model selection page'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 55, 'page_label': '56'}, page_content='Software Test Documentation \\n \\nAI Vision Tracker         44                                                                       \\n \\nChapter-6 \\nConstruction  \\n  \\n6.1 Introduction \\nThis chapter describes the testing strategy and test artifacts created to validate the No \\nVehicle Zone Anomaly Detection System, ensuring it meets functional and non -\\nfunctional requirements. \\n6.1.1 System Overview  \\nSystem components to test: Camera ingestion, preprocessing, inference engine, zone \\nchecker, database storage, notification system, and dashboard. \\n6.1.2 Test Approach \\nA mixed approach combining automated unit tests, integration tests, and manual system \\ntests. Continuous Integration (CI) will run unit tests and basic integration checks. End -to-\\nend (E2E) testing performed in staging with simulated camera streams. \\nTesting techniques: \\n‚Ä¢ Unit testing for small services (mocking external calls) \\n‚Ä¢ Integration testing across microservices (docker-compose or test Kubernetes \\nnamespace) \\n‚Ä¢ System testing with end-to-end simulated streams \\n‚Ä¢ Regression testing after model updates \\n‚Ä¢ Performance/load testing for inference throughput and API latency'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 56, 'page_label': '57'}, page_content='Software Test Documentation \\n \\n \\nAI Vision Tracker 45  \\n \\n6.1.3 Testing Objectives \\n‚Ä¢ Verify detection accuracy in no-vehicle zones (precision/recall thresholds) \\n‚Ä¢ Ensure timely alert generation (latency within acceptable SLA) \\n‚Ä¢ Ensure reliability under expected traffic (concurrency) and robustness to faults \\n(camera disconnects) \\n‚Ä¢ Validate role-based access control and data privacy (no unauthorized access) \\n‚Ä¢ Ensure that evidence frames are correctly stored and retrievable \\n \\n6.2 Test Plan \\n6.2.1 Features to be Tested \\n‚Ä¢ Camera connectivity and preprocessing pipeline \\n‚Ä¢ Object detection accuracy for supported vehicle classes \\n‚Ä¢ Tracking stability across frames \\n‚Ä¢ Zone definition, scheduling, and policy enforcement \\n‚Ä¢ Alert generation, storage, and notification \\n‚Ä¢ Dashboard display of live feed, alerts, and reports \\n‚Ä¢ Authentication and user role permissions \\n‚Ä¢ Data retention and evidence retrieval'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 57, 'page_label': '58'}, page_content='Software Test Documentation \\n \\n \\nAI Vision Tracker 46  \\n \\n6.2.2 Features not to be Tested \\n‚Ä¢ Non-essential third-party components unrelated to core functionality (e.g., \\noptional external analytics SaaS integrations), unless explicitly required by \\ndeployment environment. \\n‚Ä¢ Legacy camera firmware bugs (beyond verifying basic RTSP stream support). \\n \\n6.3 Test Cases \\nBelow are representative test cases grouped by type. \\n6.3.1 Unit Testing  \\n1. Function: pointInPolygon(polygon, point) \\no Input: polygon = square, point inside \\no Expected: True \\no Notes: Edge cases (point exactly on boundary) should be defined. \\n2. Function: isZoneActive(zone_schedule, timestamp) \\no Input: schedule with weekdays Mon-Fri 08:00-18:00; timestamp Sat 10:00 \\no Expected: False \\n3. Function: inference_postprocess(detections) \\no Input: detections with confidences [0.2,0.95,0.7] and threshold 0.5 \\no Expected: returns detections with confidences >= 0.5'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 58, 'page_label': '59'}, page_content='Software Test Documentation \\n \\n \\nAI Vision Tracker 47  \\n \\n6.3.2 Functional Testing  \\n1. Case: Vehicle enters no-vehicle zone during active period \\no Steps: Simulate detection at coordinates inside zone during active time \\no Expected: Alert created; evidence stored; notification sent to admin. \\n2. Case: Camera disconnects intermittently \\no Steps: Cut RTSP for 30s, restore \\no Expected: System logs disconnect, retries; when restored, resumes \\ningestion; no duplicate detections. \\n3. Case: Admin edits zone polygon \\no Steps: Modify polygon and save \\no Expected: New polygon used for subsequent detections; historical \\ndetections remain unchanged. \\n6.3.3 System Testing (end-to-end) \\n1. Case: Full pipeline verification \\no Steps: Stream test video with vehicles, including moments inside no-\\nvehicle zone \\no Expected: Detections appear in DB; alerts generated; dashboard shows \\nlive video + alert; evidence retrievable. \\n2. Case: Load test \\no Steps: Simulate N cameras (e.g., 50) at 1 fps and varying detection rates'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 59, 'page_label': '60'}, page_content='Software Test Documentation \\n \\n \\nAI Vision Tracker 48  \\n \\no Expected: CPU/GPU utilization within limits; average detection latency < \\ndefined SLA (e.g., 3s). Monitor metrics and ensure no message loss. \\n \\n6.3.4 Integration Testing \\n1. Case: Inference service ‚Üí Database integration \\no Steps: Mock detections forwarded to DB interface \\no Expected: DB records created with correct fields; frame_path accessible. \\n2. Case: Notification service with external SMS gateway (mock) \\no Steps: Trigger notification; observe mocked gateway responses (200 OK, \\n500 error) \\no Expected: On 200 OK, mark notification sent; on error, retry according to \\npolicy. \\n6.3.5 Validation Testing \\nValidation ensures that system fulfills stakeholder needs. \\n‚Ä¢ Acceptance criteria: Detection precision >= 0.85 and recall >= 0.80 for typical \\nscenes; alert latency <= 5 seconds in normal operation; UI must load the Alerts \\npage within 2 seconds. \\n‚Ä¢ Conduct field tests with actual cameras in real environment to validate real-world \\nperformance. \\nTest Artifacts: \\n‚Ä¢ Test Plan document (this), Test Cases (detailed templates), Test Execution Logs, \\nDefect Reports, Test Summary Report.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 60, 'page_label': '61'}, page_content='Conclusion and Future Scope \\n \\n AI Vision Tracker 49 \\n \\n  \\n \\nChapter-7 \\n Conclusion and Future Scope \\n  \\nConclusion  \\nThe No Vehicle Zone Anomaly Detection System provides an automated approach to \\ndetect and notify authorities when vehicles enter restricted/no-vehicle zones. The design \\ndescribed above targets accuracy, low-latency detection, and practical deployment \\nconsiderations (edge vs. cloud inference). The modular architecture promotes \\nmaintainability and scaling. \\nFuture Scope \\n‚ñ™ Advanced tracking and re-identification (ReID): Improve long-term tracking \\nacross multiple cameras to identify repeat offenders. \\n‚ñ™ License Plate Recognition (LPR) integration: Capture license plates for \\nenforcement and automated fines (ensure legal/privacy compliance). \\n‚ñ™ Hybrid edge-cloud inference: Run lightweight models on edge for real-time \\nresponsiveness, offload complex analytics to cloud. \\n‚ñ™ Anomaly scoring with temporal context: Use LSTM or transformer-based \\ntemporal models to reduce false positives (e.g., transient occlusions). \\n‚ñ™ Privacy-preserving methods: On-device anonymization (blurring faces) and \\nsecure storage to comply with privacy laws. \\n‚ñ™ Automated enforcement workflows: Integrate with municipal fine-management \\nsystems or law-enforcement dashboards. \\n‚ñ™ Adaptive model retraining: Periodic retraining pipelines using crowdsourced \\nlabeled data to adapt to new vehicle types or environmental changes. \\n‚ñ™ Multi-modal sensors: Add LIDAR or radar sensors in very noisy visual \\nenvironments.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 61, 'page_label': '62'}, page_content='50  \\n \\nReferences \\n \\nJournals: \\n[1] A. Singh, R. Verma, and P. Gupta, ‚ÄúReal-Time Vehicle Detection and Tracking in No-Entry Zones \\nusing Deep Learning,‚Äù IEEE Access, vol. 12, pp. 12345‚Äì12357, 2024. \\n[2] K. Wadhwa and L. Zhang, ‚ÄúEdge AI for Smart Traffic Monitoring: A Survey,‚Äù Journal of Network \\nand Computer Applications, vol. 232, pp. 1‚Äì15, 2025. \\nBooks: \\n[1] A. G√©ron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, O‚ÄôReilly Media, \\n3rd Edition, pp. 50‚Äì120, 2023. \\n[2] R. Szeliski, Computer Vision: Algorithms and Applications, Springer, 2nd Edition, pp. 245‚Äì278, \\n2022. \\nConferences: \\n[1] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, ‚ÄúYOLOv4: Optimal Speed and Accuracy of Object \\nDetection,‚Äù Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition \\n(CVPR), pp. 1103‚Äì1112, 2020. \\n[2] J. Redmon and A. Farhadi, ‚ÄúYOLOv3: An Incremental Improvement,‚Äù Proceedings of the IEEE \\nConference on Computer Vision and Pattern Recognition Workshops, pp. 1‚Äì6, 2018. \\nReports: \\n[1] NVIDIA Corporation, ‚ÄúAI Edge Deployment Guide: Real-Time Vision Processing,‚Äù Technical Report, \\nSanta Clara, USA, January 2025. \\n[2] Grand View Research, ‚ÄúGlobal Intelligent Traffic Systems Market Trends Report 2024‚Äì2030,‚Äù \\nMarket Research Report, San Francisco, December 2024. \\nProject Report / Thesis: \\n[1] Priya Sharma, ‚ÄúDeep Learning-Based Traffic Anomaly Detection Using YOLO and OpenCV,‚Äù \\nM.Tech Thesis submitted to Indian Institute of Technology Delhi, pp. 10‚Äì120, 2024. \\nWebsites: \\n[1] Ultralytics, ‚ÄúYOLOv5 Documentation,‚Äù 2025. (Website: https://github.com/ultralytics/yolov5) \\n[2] OpenCV Developers, ‚ÄúOpenCV Library Documentation,‚Äù 2024. (Website: https://docs.opencv.org) \\n[3] PyTorch Developers, ‚ÄúPyTorch Documentation,‚Äù 2025. (Website: https://pytorch.org/docs)'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 62, 'page_label': '63'}, page_content='51  \\n \\nAppendices \\nAppendix A: API Contracts and Data Exchange Format \\nA.1 Overview \\nThis appendix contains the description of APIs and data formats used for communication between \\ncomponents of the system. \\nA.2 API Endpoints \\n‚Ä¢ POST /api/detections ‚Äì Accepts JSON payloads from the inference service and stores \\ndetection data in the database. \\n‚Ä¢ GET /api/alerts ‚Äì Retrieves alert records for the dashboard with filters for date, status, and \\ncamera ID. \\n‚Ä¢ PUT /api/alerts/{id} ‚Äì Updates alert status (acknowledged/resolved). \\nA.3 Sample JSON Payload \\n{ \\n  \"camera_id\": \"CAM_001\", \\n  \"timestamp\": \"2025-10-10T14:32:17Z\", \\n  \"detections\": [ \\n    { \\n      \"track_id\": \"T123\", \\n      \"class\": \"motorcycle\", \\n      \"bbox\": [120, 80, 60, 45], \\n      \"confidence\": 0.96 \\n    } \\n  ] \\n}'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 63, 'page_label': '64'}, page_content='52  \\n \\nAppendix B: Database Table Structure \\nB.1 Overview \\nThis appendix lists the database schema used in the project for storing detection, alert, and \\nconfiguration data. \\nB.2 Table Definitions \\nB.2.1 CAMERA Table \\nField Type Description \\ncamera_id VARCHAR(50) Unique identifier of camera \\nname VARCHAR(100) Name of the camera \\nlocation_lat DECIMAL(9,6) Latitude \\nlocation_long DECIMAL(9,6) Longitude \\nip_address VARCHAR(45) IP address of camera \\nroi_polygon TEXT JSON polygon defining region of interest \\n \\nB.2.2 ZONE Table \\nField Type Description \\nzone_id INT Zone identifier \\nname VARCHAR(100) Zone name \\npolygon TEXT Zone coordinates \\nzone_type VARCHAR(50) Type of zone (no_vehicle, restricted) \\nB.2.3 ALERT Table \\nField Type Description \\nalert_id BIGINT Alert identifier'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 64, 'page_label': '65'}, page_content='53  \\n \\ndetection_id BIGINT Related detection \\ntimestamp DATETIME Alert time \\nstatus VARCHAR(20) Current status (new, acknowledged, resolved) \\n \\nB.3 Normalization \\nAll tables are normalized to Third Normal Form (3NF) to reduce redundancy and maintain data \\nintegrity.'),\n",
              " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-05T11:45:45+05:30', 'title': 'Prepared by Dr. S. B. Warkad, M.Tech. (IDC) Coordinator for Students', 'author': 'p-4', 'moddate': '2025-11-05T11:45:45+05:30', 'source': '/content/major_proj_report.pdf', 'total_pages': 66, 'page': 65, 'page_label': '66'}, page_content='54  \\n \\nAppendix C: User Interface Screens and Outputs \\nC.1 Overview \\nThis appendix includes screenshots and interface layouts of the system‚Äôs web dashboard. \\nC.2 Screens \\n‚Ä¢ Figure C.1: Login Page \\n‚Ä¢ Figure C.2: Live Camera Monitor showing multiple camera feeds \\n‚Ä¢ Figure C.3: Alerts Dashboard displaying active anomalies \\n‚Ä¢ Figure C.4: Zone Editor for defining no-vehicle zones \\nC.3 Output Examples \\n‚Ä¢ Alert Notification Message: \\n‚Äú Alert: Unauthorized vehicle detected in Zone A at 10:32 AM.‚Äù \\n‚Ä¢ Database Record Example: \\nALERT_ID=105, CAMERA_ID=CAM_002, CLASS=Car, STATUS=New, TIME=2025-10-10 10:32:12')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,       # characters per chunk\n",
        "    chunk_overlap=100,    # overlap for context continuity\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "chunks = splitter.split_documents(documents)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(chunks)} text chunks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chu71NQ8QBIM",
        "outputId": "4c4ad680-dba0-48c0-9beb-77119fa8d4ba"
      },
      "id": "chu71NQ8QBIM",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Split into 191 text chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_text(texts):\n",
        "    return embedding_model.encode(texts).tolist()\n",
        "\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "class HFEmbedding(Embeddings):\n",
        "    def embed_documents(self, texts):\n",
        "        return embed_text(texts)\n",
        "    def embed_query(self, text):\n",
        "        return embed_text([text])[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-lDfN3QQuY0",
        "outputId": "8c566d52-7410-4bbc-f456-facec77efc2e"
      },
      "id": "q-lDfN3QQuY0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import Settings\n",
        "\n",
        "client = chromadb.Client(Settings(persist_directory=\"./rag_db\"))\n",
        "collection_name = \"pdf_knowledge\"\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding_function=HFEmbedding()\n",
        ")\n",
        "\n",
        "# Add chunks to vectorstore\n",
        "texts = [chunk.page_content for chunk in chunks]\n",
        "metadatas = [chunk.metadata for chunk in chunks]\n",
        "\n",
        "vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
        "\n",
        "print(\"‚úÖ ChromaDB populated successfully with PDF chunks!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWskNXlXRKM7",
        "outputId": "db4c9e17-e0fe-4214-af01-6456226fa2bc"
      },
      "id": "KWskNXlXRKM7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ChromaDB populated successfully with PDF chunks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
      ],
      "metadata": {
        "id": "JPPbBx-_R0dW"
      },
      "id": "JPPbBx-_R0dW",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD9caRAI6oChsXLC7uDWB8FcSPcEP_oixg\""
      ],
      "metadata": {
        "id": "5zQvI15xUOJt"
      },
      "id": "5zQvI15xUOJt",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "def generate_answer(context, question):\n",
        "    prompt = f\"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "    Question: {question}\"\"\"\n",
        "\n",
        "    response = genai.GenerativeModel(\"gemini-2.5-flash\").generate_content(prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "n-sXLQf3Tvlv"
      },
      "id": "n-sXLQf3Tvlv",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(question):\n",
        "    docs = retriever.invoke(question)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "    answer = generate_answer(context, question)\n",
        "    return answer\n",
        "\n",
        "question = \"What does the document say about neural networks?\"\n",
        "response = ask_question(question)\n",
        "print(\"ü§ñ Answer:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "ahbcp3ngTwRA",
        "outputId": "86c2ae69-da97-4e8a-9b48-e28319404665"
      },
      "id": "ahbcp3ngTwRA",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Answer: The document states that the model was trained using:\n",
            "*   **Deep learning frameworks** such as TensorFlow and PyTorch.\n",
            "*   **Object detection architectures** like YOLOv8 or MobileNet SSD.\n",
            "\n",
            "It also mentions using **loss functions** (like cross-entropy and IoU loss) for optimization and running multiple **epochs** with gradually adjusted **learning rates** for stability. The experimentation phase involved testing different configurations of **batch size**, **learning rate**, and dataset splits.\n",
            "\n",
            "One of the referenced publications is titled \"Deep Learning-Based Surveillance Systems for Smart Cities.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4d5J6QPMS6tB"
      },
      "id": "4d5J6QPMS6tB"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total chunks:\", vectorstore._collection.count())\n",
        "docs = vectorstore._collection.get(limit=2)\n",
        "print(docs[\"documents\"][0][:200])  # first 200 chars of first chunk\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2_EvbGvUyEb",
        "outputId": "e3809952-21d6-40c8-8624-dbed88381b12"
      },
      "id": "Z2_EvbGvUyEb",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 191\n",
            "AI VISION TRACKER \n",
            " \n",
            "A \n",
            "Minor Project Report \n",
            "Submitted in partial fulfillment of the requirement for the award of degree of \n",
            " \n",
            "Bachelor of Technology \n",
            "In \n",
            "Computer Science & Engineering \n",
            " \n",
            "Submitted \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ZlM6PyRUyle"
      },
      "id": "9ZlM6PyRUyle",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}